{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md3lnRUI2jcw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UORiJU2O2o-N",
        "outputId": "b7408d8f-a16a-43d7-d7cb-19b48fb8168c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyeMKzhs2jc0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKyVfBvr2jc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RaFwnLY2jc2"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0TowCsV2jc3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from math import sqrt\n",
        "from math import ceil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnQwaHoU2jc4"
      },
      "outputs": [],
      "source": [
        "from torch.nn import DataParallel\n",
        "\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhFi3BMb2jc4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU-34IAs2jc5"
      },
      "source": [
        "# Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fclYy32jc8"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
        "    'PREDICT_SIZE':21, # 21일치 예측\n",
        "    'EPOCHS':5,\n",
        "    'LEARNING_RATE':1e-4,\n",
        "    'BATCH_SIZE':2048,\n",
        "    'SEED':41\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s54JTIB82jc-"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJQjzU9u2jdA"
      },
      "source": [
        "# 1. 데이터 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo5tLMN02jdA"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/LG aimers/train.csv').drop(columns=['ID', '제품'])\n",
        "price_data = pd.read_csv('/content/drive/MyDrive/LG aimers/sales.csv').drop(columns=['ID', '제품'])\n",
        "brand_data = pd.read_csv('/content/drive/MyDrive/LG aimers/brand_keyword_cnt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KhOS2tz2jdB"
      },
      "outputs": [],
      "source": [
        "tmp_date = train_data.iloc[:, 4:].replace(0, 1)\n",
        "price_data.iloc[:, 4:] = price_data.iloc[:, 4:] / tmp_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn3PfoTj2jdB"
      },
      "outputs": [],
      "source": [
        "brand_data = pd.merge(train_data.iloc[:, :4], brand_data, how='left', left_on='브랜드', right_on='브랜드')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uchIIVc22jdB"
      },
      "source": [
        "#### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kneRkwU-2jdC"
      },
      "outputs": [],
      "source": [
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    label_encoder.fit(train_data[col])\n",
        "    train_data[col] = label_encoder.transform(train_data[col])\n",
        "    price_data[col] = label_encoder.transform(price_data[col])\n",
        "    brand_data[col] = label_encoder.transform(brand_data[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFigNNO-2jdC"
      },
      "source": [
        "#### Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3yyNp5G2jdC"
      },
      "outputs": [],
      "source": [
        "def normalization(data):\n",
        "    # 숫자형 변수들의 min-max scaling을 수행하는 코드\n",
        "    numeric_cols = data.columns[4:]\n",
        "\n",
        "    # 각 column의 min 및 max 계산\n",
        "    min_values = data[numeric_cols].min(axis=1)\n",
        "    max_values = data[numeric_cols].max(axis=1)\n",
        "\n",
        "    # 각 행의 범위(max-min)를 계산하고, 범위가 0인 경우 1로 대체\n",
        "    ranges = max_values - min_values\n",
        "    ranges[ranges == 0] = 1\n",
        "\n",
        "    # min-max scaling 수행\n",
        "    data[numeric_cols] = (data[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n",
        "\n",
        "    # max와 min 값을 dictionary 형태로 저장\n",
        "    scale_min_dict = min_values.to_dict()\n",
        "    scale_max_dict = max_values.to_dict()\n",
        "    return (data, scale_min_dict, scale_max_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5W5WCi72jdC"
      },
      "outputs": [],
      "source": [
        "train_data, train_scale_min_dict, train_scale_max_dict = normalization(train_data)\n",
        "price_data, price_scale_min_dict, price_scale_max_dict = normalization(price_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmguc-UT2jdD"
      },
      "source": [
        "#### Missing Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl6lO9yi2jdD"
      },
      "outputs": [],
      "source": [
        "none_brand_idx = brand_data[brand_data.isnull().any(axis=1)].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0PIlhUD2jdD"
      },
      "outputs": [],
      "source": [
        "drop_train_data = train_data.loc[none_brand_idx]\n",
        "drop_price_data = price_data.loc[none_brand_idx]\n",
        "drop_brand_data = brand_data.loc[none_brand_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7xQa3Cd2jdD"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(none_brand_idx)\n",
        "price_data = price_data.drop(none_brand_idx)\n",
        "brand_data = brand_data.drop(none_brand_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "eyBA1noQ2jdE",
        "outputId": "ff58a017-bff9-4717-c9d8-ce6d02826986"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       대분류  중분류  소분류   브랜드  2022-01-01  2022-01-02  2022-01-03  2022-01-04  \\\n",
              "494      1    3   18    95    0.000000    0.000000    0.000000    0.000000   \n",
              "1249     4   10   51   246    0.142857    0.428571    0.714286    0.714286   \n",
              "1261     1    1    5   250    1.000000    0.814815    0.759259    0.851852   \n",
              "1262     1    1    5   250    0.000000    0.000000    0.000000    0.000000   \n",
              "1477     0    0    2   303    0.000000    0.000000    0.000000    0.000000   \n",
              "...    ...  ...  ...   ...         ...         ...         ...         ...   \n",
              "15713    1    6   39  3142    0.032258    0.032258    0.032258    0.010753   \n",
              "15714    1    6   37  3142    0.033333    0.033333    0.033333    0.033333   \n",
              "15764    0    0    0  3149    0.000000    0.000000    0.000000    0.000000   \n",
              "15765    0    0    0  3149    0.000000    0.000000    0.000000    0.000000   \n",
              "15766    0    0    0  3149    0.000000    0.000000    0.000000    0.000000   \n",
              "\n",
              "       2022-01-05  2022-01-06  ...  2023-03-26  2023-03-27  2023-03-28  \\\n",
              "494      0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "1249     0.285714    0.285714  ...    0.000000    0.000000    0.000000   \n",
              "1261     0.944444    0.833333  ...    0.000000    0.000000    0.000000   \n",
              "1262     0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "1477     0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "...           ...         ...  ...         ...         ...         ...   \n",
              "15713    0.010753    0.010753  ...    0.010753    0.010753    0.032258   \n",
              "15714    0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "15764    0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "15765    0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "15766    0.000000    0.000000  ...    0.000000    0.000000    0.000000   \n",
              "\n",
              "       2023-03-29  2023-03-30  2023-03-31  2023-04-01  2023-04-02  2023-04-03  \\\n",
              "494      0.004883    0.002930    0.009766    0.003906    0.004883    0.009766   \n",
              "1249     0.000000    0.142857    0.285714    0.000000    0.000000    0.428571   \n",
              "1261     0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "1262     0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "1477     0.064516    0.451613    0.645161    0.161290    0.451613    0.419355   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "15713    0.032258    0.021505    0.000000    0.043011    0.021505    0.032258   \n",
              "15714    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "15764    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "15765    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "15766    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "\n",
              "       2023-04-04  \n",
              "494      0.002930  \n",
              "1249     0.000000  \n",
              "1261     0.000000  \n",
              "1262     0.000000  \n",
              "1477     0.193548  \n",
              "...           ...  \n",
              "15713    0.010753  \n",
              "15714    0.000000  \n",
              "15764    0.000000  \n",
              "15765    0.000000  \n",
              "15766    0.000000  \n",
              "\n",
              "[208 rows x 463 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d28b593-0f25-4596-883d-91d49fd9d38c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>대분류</th>\n",
              "      <th>중분류</th>\n",
              "      <th>소분류</th>\n",
              "      <th>브랜드</th>\n",
              "      <th>2022-01-01</th>\n",
              "      <th>2022-01-02</th>\n",
              "      <th>2022-01-03</th>\n",
              "      <th>2022-01-04</th>\n",
              "      <th>2022-01-05</th>\n",
              "      <th>2022-01-06</th>\n",
              "      <th>...</th>\n",
              "      <th>2023-03-26</th>\n",
              "      <th>2023-03-27</th>\n",
              "      <th>2023-03-28</th>\n",
              "      <th>2023-03-29</th>\n",
              "      <th>2023-03-30</th>\n",
              "      <th>2023-03-31</th>\n",
              "      <th>2023-04-01</th>\n",
              "      <th>2023-04-02</th>\n",
              "      <th>2023-04-03</th>\n",
              "      <th>2023-04-04</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>95</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.002930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>51</td>\n",
              "      <td>246</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>250</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.814815</td>\n",
              "      <td>0.759259</td>\n",
              "      <td>0.851852</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>303</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.645161</td>\n",
              "      <td>0.161290</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.193548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15713</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>39</td>\n",
              "      <td>3142</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043011</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.010753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15714</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>37</td>\n",
              "      <td>3142</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15764</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15765</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15766</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows × 463 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d28b593-0f25-4596-883d-91d49fd9d38c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d28b593-0f25-4596-883d-91d49fd9d38c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d28b593-0f25-4596-883d-91d49fd9d38c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e4ddf14a-dd46-4dc0-a016-79cd99637b6f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4ddf14a-dd46-4dc0-a016-79cd99637b6f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e4ddf14a-dd46-4dc0-a016-79cd99637b6f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "drop_train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwGbhEmm2jdE"
      },
      "source": [
        "#### validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0acvBL0P2jdE"
      },
      "outputs": [],
      "source": [
        "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE'], feature_size = 4):\n",
        "\n",
        "    num_rows = len(data)\n",
        "    window_size = train_size + predict_size\n",
        "    series_size = len(data.iloc[0, feature_size:]) - window_size + 1\n",
        "\n",
        "    input_data = np.empty((num_rows * series_size, train_size, len(data.iloc[0, :feature_size]) +1))\n",
        "    target_data = np.empty((num_rows * series_size, predict_size))\n",
        "\n",
        "    for i in tqdm(range(num_rows)):\n",
        "        encode_info = np.array(data.iloc[i, :feature_size])\n",
        "        sales_data = np.array(data.iloc[i, feature_size:])\n",
        "\n",
        "        for j in range(len(sales_data) - window_size + 1):\n",
        "            window = sales_data[j : j + window_size]\n",
        "\n",
        "\n",
        "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
        "            input_data[i * series_size + j] = temp_data\n",
        "            target_data[i * series_size + j] = window[train_size:]\n",
        "\n",
        "    return input_data, target_data, series_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmTzyNS72jdE"
      },
      "outputs": [],
      "source": [
        "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], feature_size=4):\n",
        "\n",
        "    num_rows = len(data)\n",
        "\n",
        "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :feature_size]) + 1))\n",
        "\n",
        "    for i in tqdm(range(num_rows)):\n",
        "        encode_info = np.array(data.iloc[i, :feature_size])\n",
        "        sales_data = np.array(data.iloc[i, -train_size:])\n",
        "\n",
        "        window = sales_data[-train_size : ]\n",
        "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
        "        input_data[i] = temp_data\n",
        "\n",
        "    return input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz8Pfc8Z2jdF"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    'sales_data' : train_data,\n",
        "    'brand_data' : brand_data,\n",
        "    'price_data' : price_data\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "y2W8Hz982jdG",
        "outputId": "119fb514-5c56-4906-9434-ac257e8c3183"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-9f73ead3629f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-69308cd1a78f>\u001b[0m in \u001b[0;36mmake_train_data\u001b[0;34m(data, train_size, predict_size, feature_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredict_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mseries_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseries_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iloc'"
          ]
        }
      ],
      "source": [
        "train_input, train_target, series_size = make_train_data(data)\n",
        "test_input = make_predict_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fobTPfZO2jdG"
      },
      "outputs": [],
      "source": [
        "np.save('train_input.npy', train_input)\n",
        "np.save('train_target.npy', train_target)\n",
        "np.save('test_input.npy', test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kVm3QVd2jdH"
      },
      "outputs": [],
      "source": [
        "train_input = np.load('train_input.npy')\n",
        "train_target = np.load('train_target.npy')\n",
        "test_input = np.load('test_input.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, train_target, series_size = make_train_data(train_data)\n",
        "test_input = make_predict_data(train_data)"
      ],
      "metadata": {
        "id": "Zj6AigZl4C95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF8aQnZn2jdH"
      },
      "outputs": [],
      "source": [
        "def train_val_split(data, input_data, target_data, series_size, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
        "\n",
        "    val_index = sorted(random.sample(range(series_size),int(series_size*0.2)))\n",
        "\n",
        "    num_rows=len(data)\n",
        "\n",
        "    val_inputs = [] # 일 순서대로 담을 예정\n",
        "    val_targets = []\n",
        "    for i in tqdm(val_index):\n",
        "        inputs = np.empty((num_rows, train_size, input_data.shape[2]))\n",
        "        targets = np.empty((num_rows, predict_size))\n",
        "        for j in range(num_rows):\n",
        "            inputs[j] = input_data[j * series_size + i]\n",
        "            targets[j] = target_data[j * series_size + i]\n",
        "        val_inputs.append(inputs)\n",
        "        val_targets.append(targets)\n",
        "\n",
        "    train_series_size = series_size - len(val_index)\n",
        "\n",
        "    train_input = np.empty((num_rows * train_series_size, train_size, input_data.shape[2]))\n",
        "    train_target = np.empty((num_rows * train_series_size, predict_size))\n",
        "\n",
        "    # train 데이터 생성\n",
        "    k = 0\n",
        "    for i in tqdm(range(series_size)):\n",
        "        if i not in val_index:\n",
        "            for j in range(num_rows):\n",
        "                train_input[k] = input_data[j * series_size + i]\n",
        "                train_target[k] = target_data[j * series_size + i]\n",
        "                k += 1\n",
        "\n",
        "    return train_input, train_target, val_inputs, val_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmqZIGq02jdI"
      },
      "outputs": [],
      "source": [
        "# Train / Validation Split\n",
        "train_input, train_target, val_inputs, val_targets = train_val_split(train_data, train_input, train_target, series_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_E7NRAz2jdI"
      },
      "outputs": [],
      "source": [
        "train_input.shape, train_target.shape, test_input.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyMW0ajY2jdI"
      },
      "outputs": [],
      "source": [
        "val_inputs[0].shape, val_targets[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pURqie212jdI"
      },
      "outputs": [],
      "source": [
        "len(val_inputs), len(val_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7Rqs5o2jdJ"
      },
      "source": [
        "#### Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QXUnts02jdR"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.Y is not None:\n",
        "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
        "        return torch.Tensor(self.X[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwtWHPp62jdS"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_input, train_target)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n4qWL8y2jdS"
      },
      "source": [
        "# 2. 모델 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeIGNzEA2jdT"
      },
      "source": [
        "#### Attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMCell(tf.keras.Model):\n",
        "    def __init__(self, hidden_dim, kernel_size, bias):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            filters = 4 * self.hidden_dim,\n",
        "            kernel_size = self.kernel_size,\n",
        "            padding = 'same',\n",
        "            use_bias = self.bias\n",
        "        )\n",
        "\n",
        "    def call(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "        combined = tf.concat([input_tensor, h_cur], axis=3)\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = tf.split(combined_conv, num_or_size_splits=4, axis=-1)\n",
        "        i = tf.keras.activations.sigmoid(cc_i)\n",
        "        f = tf.keras.activations.sigmoid(cc_f)\n",
        "        o = tf.keras.activations.sigmoid(cc_o)\n",
        "        g = tf.keras.activations.tanh(cc_g)\n",
        "\n",
        "        c_next = f*c_cur+i*g\n",
        "        h_next = o*tf.keras.activations.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (tf.zeros([batch_size, height, width, self.hidden_dim]),\n",
        "                tf.zeros([batch_size, height, width, self.hidden_dim]))"
      ],
      "metadata": {
        "id": "-rmrxwfx4TCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, hidden, enc_num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_num_layers = enc_num_layers\n",
        "        self.encoder_input_convlstm = ConvLSTMCell(\n",
        "            hidden_dim=hidden,\n",
        "            kernel_size=(3, 3),\n",
        "            bias=True\n",
        "        )\n",
        "        if self.enc_num_layers is not None:\n",
        "            self.hidden_encoder_layers = [\n",
        "                ConvLSTMCell(\n",
        "                    hidden_dim=hidden,\n",
        "                    kernel_size=(3, 3),\n",
        "                    bias=True\n",
        "                ) for _ in range(self.enc_num_layers)\n",
        "            ]\n",
        "\n",
        "    def call(self, enc_input):\n",
        "        h_t, c_t = self.init_hidden(enc_input, 'seq')\n",
        "        if self.enc_num_layers is not None:\n",
        "            hidden_h_t = []\n",
        "            hidden_c_t = []\n",
        "            for i in range(self.enc_num_layers):\n",
        "                hidden_h_t += [self.init_hidden(h_t, i)[0]]\n",
        "                hidden_c_t += [self.init_hidden(h_t, i)[1]]\n",
        "\n",
        "        seq_len = enc_input.shape[1]\n",
        "        for t in range(seq_len):\n",
        "            h_t, c_t = self.encoder_input_convlstm(\n",
        "                input_tensor=enc_input[:, t, :, :, :],\n",
        "                cur_state=[h_t, c_t]\n",
        "            )\n",
        "            input_tensor = h_t\n",
        "            if self.enc_num_layers is not None:\n",
        "                for i in range(self.enc_num_layers):\n",
        "                    hidden_h_t[i], hidden_c_t[i] = self.hidden_encoder_layers[i](\n",
        "                        input_tensor=input_tensor,\n",
        "                        cur_state=[hidden_h_t[i], hidden_c_t[i]]\n",
        "                    )\n",
        "                    input_tensor = hidden_h_t[i]\n",
        "\n",
        "        if self.enc_num_layers is not None:\n",
        "            return hidden_h_t[-1], hidden_c_t[-1]\n",
        "        else:\n",
        "            return h_t, c_t\n",
        "\n",
        "    def init_hidden(self, input_tensor, seq):\n",
        "        if seq == 'seq':\n",
        "            b, seq_len, h, w, _ = input_tensor.shape\n",
        "            h_t, c_t = self.encoder_input_convlstm.init_hidden(\n",
        "                batch_size=b,\n",
        "                image_size=(h, w)\n",
        "            )\n",
        "        else:\n",
        "            b, h, w, _ = input_tensor.shape\n",
        "            h_t, c_t = self.hidden_encoder_layers[seq].init_hidden(\n",
        "                batch_size=b,\n",
        "                image_size=(h, w)\n",
        "            )\n",
        "        return h_t, c_t"
      ],
      "metadata": {
        "id": "v18T9wex4aCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_enc_input_data = next(iter(train_data))[0]\n",
        "sample_encoder = Encoder(16, 1)\n",
        "enc_output = sample_encoder(sample_enc_input_data)\n",
        "enc_output[0].shape, enc_output[1].shape"
      ],
      "metadata": {
        "id": "U9oykx3p4drW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, hidden, dec_num_layers=1, future_len=12):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_num_layers = dec_num_layers\n",
        "        self.future_len = future_len\n",
        "        self.decoder_input_convlstm = ConvLSTMCell(\n",
        "            hidden_dim=hidden,\n",
        "            kernel_size=(3, 3),\n",
        "            bias=True\n",
        "        )\n",
        "        if self.dec_num_layers is not None:\n",
        "            self.hidden_decoder_layers = [\n",
        "                ConvLSTMCell(\n",
        "                    hidden_dim=hidden,\n",
        "                    kernel_size=(3, 3),\n",
        "                    bias=True\n",
        "                ) for _ in range(dec_num_layers)\n",
        "            ]\n",
        "\n",
        "        self.decoder_output_layer = tf.keras.layers.Conv2D(\n",
        "            filters=1,\n",
        "            kernel_size=(3,3),\n",
        "            padding='same',\n",
        "            activation='sigmoid'\n",
        "        )\n",
        "\n",
        "    def call(self, enc_output):\n",
        "        if self.dec_num_layers is not None:\n",
        "            hidden_h_t = []\n",
        "            hidden_c_t = []\n",
        "            for i in range(self.dec_num_layers):\n",
        "                hidden_h_t += [self.init_hidden(enc_output[0], i)[0]]\n",
        "                hidden_c_t += [self.init_hidden(enc_output[0], i)[1]]\n",
        "\n",
        "        outputs = []\n",
        "        input_tensor = enc_output[0]\n",
        "        h_t, c_t = self.init_hidden(input_tensor, 'seq')\n",
        "        for t in range(self.future_len):\n",
        "            h_t, c_t=self.decoder_input_convlstm(\n",
        "                input_tensor=input_tensor,\n",
        "                cur_state=[h_t, c_t]\n",
        "            )\n",
        "            input_tensor = h_t\n",
        "            if self.dec_num_layers is not None:\n",
        "                for i in range(self.dec_num_layers):\n",
        "                    hidden_h_t[i], hidden_c_t[i] = self.hidden_decoder_layers[i](\n",
        "                        input_tensor=input_tensor,\n",
        "                        cur_state=[hidden_h_t[i], hidden_c_t[i]]\n",
        "                    )\n",
        "                    input_tensor=hidden_h_t[i]\n",
        "                output = self.decoder_output_layer(hidden_h_t[-1])\n",
        "            else:\n",
        "                output = self.decoder_output_layer(h_t)\n",
        "            outputs += [output]\n",
        "        outputs = tf.stack(outputs, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def init_hidden(self, input_tensor, seq):\n",
        "        if seq == 'seq':\n",
        "            b, h, w, _ = input_tensor.shape\n",
        "            h_t, c_t = self.decoder_input_convlstm.init_hidden(\n",
        "                batch_size=b,\n",
        "                image_size=(h, w)\n",
        "            )\n",
        "        else:\n",
        "            b, h, w, _ = input_tensor.shape\n",
        "            h_t, c_t = self.hidden_decoder_layers[seq].init_hidden(\n",
        "                batch_size=b,\n",
        "                image_size=(h, w)\n",
        "            )\n",
        "        return h_t, c_t"
      ],
      "metadata": {
        "id": "W9gM_5L-4iyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder = Decoder(16)\n",
        "dec_output = sample_decoder(enc_output)\n",
        "dec_output.shape"
      ],
      "metadata": {
        "id": "CrK32uIX4lHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwwcvG1H2jdU"
      },
      "source": [
        "#### Cross Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_JwBb_62jdV"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    '''\n",
        "    The decoder layer of Crossformer, each layer will make a prediction at its scale\n",
        "    '''\n",
        "    def __init__(self, seg_len, d_model, n_heads, d_ff=None, dropout=0.1, out_seg_num = 10, factor = 10):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = TwoStageAttentionLayer(out_seg_num, factor, d_model, n_heads, \\\n",
        "                                d_ff, dropout)\n",
        "        self.cross_attention = AttentionLayer(d_model, n_heads, dropout = dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.MLP1 = nn.Sequential(nn.Linear(d_model, d_model),\n",
        "                                nn.GELU(),\n",
        "                                nn.Linear(d_model, d_model))\n",
        "        self.linear_pred = nn.Linear(d_model, seg_len)\n",
        "\n",
        "    def forward(self, x, cross):\n",
        "        '''\n",
        "        x: the output of last decoder layer\n",
        "        cross: the output of the corresponding encoder layer\n",
        "        '''\n",
        "\n",
        "        batch = x.shape[0]\n",
        "        x = self.self_attention(x)\n",
        "        x = rearrange(x, 'b ts_d out_seg_num d_model -> (b ts_d) out_seg_num d_model')\n",
        "\n",
        "        cross = rearrange(cross, 'b ts_d in_seg_num d_model -> (b ts_d) in_seg_num d_model')\n",
        "        tmp = self.cross_attention(\n",
        "            x, cross, cross,\n",
        "        )\n",
        "        x = x + self.dropout(tmp)\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.MLP1(y)\n",
        "        dec_output = self.norm2(x+y)\n",
        "\n",
        "        dec_output = rearrange(dec_output, '(b ts_d) seg_dec_num d_model -> b ts_d seg_dec_num d_model', b = batch)\n",
        "        layer_predict = self.linear_pred(dec_output)\n",
        "        layer_predict = rearrange(layer_predict, 'b out_d seg_num seg_len -> b (out_d seg_num) seg_len')\n",
        "\n",
        "        return dec_output, layer_predict\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    The decoder of Crossformer, making the final prediction by adding up predictions at each scale\n",
        "    '''\n",
        "    def __init__(self, seg_len, d_layers, d_model, n_heads, d_ff, dropout,\\\n",
        "                router=False, out_seg_num = 10, factor=10):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.router = router\n",
        "        self.decode_layers = nn.ModuleList()\n",
        "        for i in range(d_layers):\n",
        "            self.decode_layers.append(DecoderLayer(seg_len, d_model, n_heads, d_ff, dropout, \\\n",
        "                                        out_seg_num, factor))\n",
        "\n",
        "    def forward(self, x, cross):\n",
        "        final_predict = None\n",
        "        i = 0\n",
        "\n",
        "        ts_d = x.shape[1]\n",
        "        for layer in self.decode_layers:\n",
        "            cross_enc = cross[i]\n",
        "            x, layer_predict = layer(x,  cross_enc)\n",
        "            if final_predict is None:\n",
        "                final_predict = layer_predict\n",
        "            else:\n",
        "                final_predict = final_predict + layer_predict\n",
        "            i += 1\n",
        "\n",
        "        final_predict = rearrange(final_predict, 'b (out_d seg_num) seg_len -> b (seg_num seg_len) out_d', out_d = ts_d)\n",
        "\n",
        "        return final_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DDKiJcA2jdV"
      },
      "source": [
        "#### Cross Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4sA-LU32jdW"
      },
      "outputs": [],
      "source": [
        "class DSW_embedding(nn.Module):\n",
        "    def __init__(self, seg_len, d_model):\n",
        "        super(DSW_embedding, self).__init__()\n",
        "        self.seg_len = seg_len\n",
        "\n",
        "        self.linear = nn.Linear(seg_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, ts_len, ts_dim = x.shape\n",
        "\n",
        "        x_segment = rearrange(x, 'b (seg_num seg_len) d -> (b d seg_num) seg_len', seg_len = self.seg_len)\n",
        "        x_embed = self.linear(x_segment)\n",
        "        x_embed = rearrange(x_embed, '(b d seg_num) d_model -> b d seg_num d_model', b = batch, d = ts_dim)\n",
        "\n",
        "        return x_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WMmOKe12jdW"
      },
      "source": [
        "#### Cross Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGz0K9GY2jdX"
      },
      "outputs": [],
      "source": [
        "class SegMerging(nn.Module):\n",
        "    '''\n",
        "    Segment Merging Layer.\n",
        "    The adjacent `win_size' segments in each dimension will be merged into one segment to\n",
        "    get representation of a coarser scale\n",
        "    we set win_size = 2 in our paper\n",
        "    '''\n",
        "    def __init__(self, d_model, win_size, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.win_size = win_size\n",
        "        self.linear_trans = nn.Linear(win_size * d_model, d_model)\n",
        "        self.norm = norm_layer(win_size * d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, ts_d, L, d_model\n",
        "        \"\"\"\n",
        "        batch_size, ts_d, seg_num, d_model = x.shape\n",
        "        pad_num = seg_num % self.win_size\n",
        "        if pad_num != 0:\n",
        "            pad_num = self.win_size - pad_num\n",
        "            x = torch.cat((x, x[:, :, -pad_num:, :]), dim = -2)\n",
        "\n",
        "        seg_to_merge = []\n",
        "        for i in range(self.win_size):\n",
        "            seg_to_merge.append(x[:, :, i::self.win_size, :])\n",
        "        x = torch.cat(seg_to_merge, -1)  # [B, ts_d, seg_num/win_size, win_size*d_model]\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.linear_trans(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class scale_block(nn.Module):\n",
        "    '''\n",
        "    We can use one segment merging layer followed by multiple TSA layers in each scale\n",
        "    the parameter `depth' determines the number of TSA layers used in each scale\n",
        "    We set depth = 1 in the paper\n",
        "    '''\n",
        "    def __init__(self, win_size, d_model, n_heads, d_ff, depth, dropout, \\\n",
        "                    seg_num = 10, factor=10):\n",
        "        super(scale_block, self).__init__()\n",
        "\n",
        "        if (win_size > 1):\n",
        "            self.merge_layer = SegMerging(d_model, win_size, nn.LayerNorm)\n",
        "        else:\n",
        "            self.merge_layer = None\n",
        "\n",
        "        self.encode_layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.encode_layers.append(TwoStageAttentionLayer(seg_num, factor, d_model, n_heads, \\\n",
        "                                                        d_ff, dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, ts_dim, _, _ = x.shape\n",
        "\n",
        "        if self.merge_layer is not None:\n",
        "            x = self.merge_layer(x)\n",
        "\n",
        "        for layer in self.encode_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    The Encoder of Crossformer.\n",
        "    '''\n",
        "    def __init__(self, e_blocks, win_size, d_model, n_heads, d_ff, block_depth, dropout,\n",
        "                in_seg_num = 10, factor=10):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encode_blocks = nn.ModuleList()\n",
        "\n",
        "        self.encode_blocks.append(scale_block(1, d_model, n_heads, d_ff, block_depth, dropout,\\\n",
        "                                            in_seg_num, factor))\n",
        "        for i in range(1, e_blocks):\n",
        "            self.encode_blocks.append(scale_block(win_size, d_model, n_heads, d_ff, block_depth, dropout,\\\n",
        "                                            ceil(in_seg_num/win_size**i), factor))\n",
        "\n",
        "    def forward(self, x):\n",
        "        encode_x = []\n",
        "        encode_x.append(x)\n",
        "\n",
        "        for block in self.encode_blocks:\n",
        "            x = block(x)\n",
        "            encode_x.append(x)\n",
        "\n",
        "        return encode_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lBPlFRq2jdX"
      },
      "source": [
        "#### CrossFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8cRxqoK2jdX"
      },
      "outputs": [],
      "source": [
        "class Crossformer(nn.Module):\n",
        "    def __init__(self, data_dim, in_len, out_len, seg_len, win_size=4,\n",
        "                factor=10, d_model=512, d_ff = 1024, n_heads=8, e_layers=3,\n",
        "                dropout=0.0, baseline = False, device=torch.device('cuda:0')):\n",
        "        super(Crossformer, self).__init__()\n",
        "        self.data_dim = data_dim\n",
        "        self.in_len = in_len\n",
        "        self.out_len = out_len\n",
        "        self.seg_len = seg_len\n",
        "        self.merge_win = win_size\n",
        "\n",
        "        self.baseline = baseline\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # The padding operation to handle invisible sgemnet length\n",
        "        self.pad_in_len = ceil(1.0 * in_len / seg_len) * seg_len\n",
        "        self.pad_out_len = ceil(1.0 * out_len / seg_len) * seg_len\n",
        "        self.in_len_add = self.pad_in_len - self.in_len\n",
        "\n",
        "        # Embedding\n",
        "        self.enc_value_embedding = DSW_embedding(seg_len, d_model)\n",
        "        self.enc_pos_embedding = nn.Parameter(torch.randn(1, data_dim, (self.pad_in_len // seg_len), d_model))\n",
        "        self.pre_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(e_layers, win_size, d_model, n_heads, d_ff, block_depth = 1, \\\n",
        "                                    dropout = dropout,in_seg_num = (self.pad_in_len // seg_len), factor = factor)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec_pos_embedding = nn.Parameter(torch.randn(1, data_dim, (self.pad_out_len // seg_len), d_model))\n",
        "        self.decoder = Decoder(seg_len, e_layers + 1, d_model, n_heads, d_ff, dropout, \\\n",
        "                                    out_seg_num = (self.pad_out_len // seg_len), factor = factor)\n",
        "\n",
        "\n",
        "    def forward(self, x_seq):\n",
        "        if (self.baseline):\n",
        "            base = x_seq.mean(dim = 1, keepdim = True)\n",
        "        else:\n",
        "            base = 0\n",
        "        batch_size = x_seq.shape[0]\n",
        "        if (self.in_len_add != 0):\n",
        "            x_seq = torch.cat((x_seq[:, :1, :].expand(-1, self.in_len_add, -1), x_seq), dim = 1)\n",
        "\n",
        "        x_seq = self.enc_value_embedding(x_seq)\n",
        "        x_seq += self.enc_pos_embedding\n",
        "        x_seq = self.pre_norm(x_seq)\n",
        "\n",
        "        enc_out = self.encoder(x_seq)\n",
        "\n",
        "        dec_in = repeat(self.dec_pos_embedding, 'b ts_d l d -> (repeat b) ts_d l d', repeat = batch_size)\n",
        "        predict_y = self.decoder(dec_in, enc_out)\n",
        "\n",
        "\n",
        "        return base + predict_y[:, :self.out_len, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5VvmMKs2jdY"
      },
      "outputs": [],
      "source": [
        "# 파라미터 설정\n",
        "\n",
        "data_dim = 7\n",
        "in_len = CFG['TRAIN_WINDOW_SIZE']\n",
        "out_len = CFG['PREDICT_SIZE']\n",
        "seg_len = 6\n",
        "win_size = 2\n",
        "factor = 10\n",
        "d_model = 256    # hidden_size\n",
        "d_ff = 512       # dimension of MLP in transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs44yOyU2jdY"
      },
      "outputs": [],
      "source": [
        "model = Crossformer(data_dim, in_len, out_len, seg_len, win_size, factor, d_model, d_ff)\n",
        "model = nn.DataParallel(model, device_ids = [0, 1, 2, 3])\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxgV_w482jdZ"
      },
      "source": [
        "# 3. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHpSsWrp2jdZ"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_loader, device):\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss().to(device)\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(1, CFG['EPOCHS']+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        train_mae = []\n",
        "        for X, Y in tqdm(iter(train_loader)):\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(X)\n",
        "            loss = criterion(output[:, :, -3], Y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "        val_score = []\n",
        "        for i in tqdm(range(len(val_inputs))):\n",
        "            val_dataset = CustomDataset(val_inputs[i], val_targets[i])\n",
        "            val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "            val_score.append(validation(model, val_loader, device))\n",
        "\n",
        "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{np.mean(val_score):.5f}]')\n",
        "\n",
        "        if best_score < val_score:\n",
        "            best_score = val_score\n",
        "            best_model = model\n",
        "            print('Model Saved')\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scJ8ASMJ2jdZ"
      },
      "outputs": [],
      "source": [
        "indexs_bigcat={}\n",
        "for bigcat in train_data['대분류'].unique():\n",
        "    indexs_bigcat[bigcat] = list(train_data.loc[train_data['대분류']==bigcat].index)\n",
        "\n",
        "indexs_bigcat.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwWkti6l2jda"
      },
      "outputs": [],
      "source": [
        "def PSFA(pred, target):\n",
        "    PSFA = 1\n",
        "    mapping_ids = {j:i for i, j in enumerate(train_data.index)}\n",
        "    for cat in range(5):\n",
        "        ids = indexs_bigcat[cat]\n",
        "        ids = list(pd.Series(ids).map(mapping_ids))\n",
        "        for day in range(21):\n",
        "            total_sell = np.sum(target[ids, day]) # day별 총 판매량\n",
        "            pred_values = pred[ids, day] # day별 예측 판매량\n",
        "            target_values = target[ids, day] # day별 실제 판매량\n",
        "\n",
        "            # 실제 판매와 예측 판매가 같은 경우 오차가 없는 것으로 간주\n",
        "            denominator = np.maximum(target_values, pred_values)\n",
        "            diffs = np.where(denominator!=0, np.abs(target_values - pred_values) / denominator, 0)\n",
        "\n",
        "            if total_sell != 0:\n",
        "                sell_weights = target_values / total_sell  # Item별 day 총 판매량 내 비중\n",
        "            else:\n",
        "                sell_weights = np.ones_like(target_values) / len(ids)  # 1 / len(ids)로 대체\n",
        "\n",
        "            if not np.isnan(diffs).any():  # diffs에 NaN이 없는 경우에만 PSFA 값 업데이트\n",
        "                PSFA -= np.sum(diffs * sell_weights) / (21 * 5)\n",
        "\n",
        "\n",
        "    return PSFA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ai80m3d2jda"
      },
      "outputs": [],
      "source": [
        "def validation(model, val_loader, device):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    target = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, Y in iter(val_loader):\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            Y = Y.cpu().numpy()\n",
        "            target.extend(Y)\n",
        "\n",
        "            output = model(X)\n",
        "            output = output[:, :, -3].cpu().numpy()\n",
        "            pred.extend(output)\n",
        "\n",
        "    pred = np.array(pred)\n",
        "    target = np.array(target)\n",
        "\n",
        "    # 추론 결과를 inverse scaling\n",
        "    for i, idx in enumerate(train_data.index):\n",
        "        x = pred[i, :] * (train_scale_max_dict[idx] - train_scale_min_dict[idx]) + train_scale_min_dict[idx]\n",
        "        pred[i, :] = np.maximum(0, x)\n",
        "        target[i, :] = target[i, :] * (train_scale_max_dict[idx] - train_scale_min_dict[idx]) + train_scale_min_dict[idx]\n",
        "\n",
        "    # 결과 후처리\n",
        "    pred = np.round(pred, 0).astype(int)\n",
        "    target = np.round(target, 0).astype(int)\n",
        "\n",
        "    return PSFA(pred, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "VGBoHLsS2jda"
      },
      "outputs": [],
      "source": [
        "infer_model = train(model, optimizer, train_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VXAYFwQ2jdb"
      },
      "outputs": [],
      "source": [
        "torch.save(infer_model, 'transformer_model_epoch10.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfT5FMv02jdb"
      },
      "outputs": [],
      "source": [
        "infer_model = torch.load('transformer_model_epoch10.pth', map_location=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khbueAvC2jdc"
      },
      "source": [
        "# 4. 제출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jvCMABe2jdc"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(test_input, None)\n",
        "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqF7anrm2jdd"
      },
      "outputs": [],
      "source": [
        "def inference(model, test_loader, device):\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X in tqdm(iter(test_loader)):\n",
        "            X = X.to(device)\n",
        "\n",
        "            output = model(X)\n",
        "\n",
        "            # 모델 출력인 output을 CPU로 이동하고 numpy 배열로 변환\n",
        "            output = output.cpu().numpy()\n",
        "\n",
        "            predictions.extend(output[:, :, -3])\n",
        "\n",
        "    return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSe5W_q-2jdd"
      },
      "outputs": [],
      "source": [
        "pred = inference(infer_model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGXEn9jr2jde"
      },
      "outputs": [],
      "source": [
        "# 추론 결과를 inverse scaling\n",
        "for i, idx in enumerate(train_data.index):\n",
        "    x = pred[i, :] * (train_scale_max_dict[idx] - train_scale_min_dict[idx]) + train_scale_min_dict[idx]\n",
        "    pred[i, :] = np.maximum(0, x)\n",
        "\n",
        "# 결과 후처리\n",
        "pred = np.round(pred, 0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ioonJs2jdf"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')\n",
        "submit.iloc[train_data.index, 1:] = pred\n",
        "submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmR_Rdbv2jdg"
      },
      "outputs": [],
      "source": [
        "baseline_submit = pd.read_csv('baseline_submit.csv')\n",
        "submit.iloc[none_brand_idx,1:] = baseline_submit.iloc[none_brand_idx,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIJQ0S242jdh"
      },
      "outputs": [],
      "source": [
        "submit.to_csv('transformer_model_epoch10_submit.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}